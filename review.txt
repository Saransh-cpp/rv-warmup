Runtime verification (RV) is a newly emerging field of study to detect bugs in code at runtime. The bugs are detected by monitoring the trace of a program's execution and running them against formal specifications (specs).

Till now, the main focus of studies on RV has been to improve the efficiency of the process by innovating new algorithms and bringing down the runtime overhead of RV software. There have been scant to no large-scale, in-depth studies on the effectiveness of defined specs, which this paper aims to do.

The authors conduct a large-scale experiment, measure the RV overhead, classify violations through a meticulous procedure, submit fixes, and leave suggestions for the future of RV. They run JavaMOP (RV software) on 200 open-source projects with manual and automatically generated tests and run the execution against 182 manual + 17 automatic mined specs. Next, they measure the RV overhead and sample a fraction of static violations for manual inspection. The static violations are unique violations, and the sampled ones are more likely to be true functional bugs and easier to inspect than the skipped ones. The sampled violations, classified as True Bug (TB), False Alarm (FA), or Hard To Inspect (HTI), are, for the first time in a study, run through project maintainers to confirm the classification, making this study even more effective.

The paper captures three main results:
    1. RV algorithms and software have decent runtime overhead and are efficient enough to be used in the development/debugging process (4.3x average RV overhead).
    2. The existing specs are good enough to find bugs, and developers appreciate such fixes (provided they are not spammed with poorly thought-out bug reports and fixes).
    3. The False Alarm Rate is alarmingly high. 69/200 projects result in a 100% FAR, and the Iterator_HasNext (the most commonly used spec in RV studies) spec has a FAR of 97.40%. Java libraries have less FAR than projects, as they are user-facing and better tested.

The RV software still cannot be used in a production pipeline because of the runtime overhead. I observed that the RV software used in this paper has been stale for a few years (including broken links), with no active development on GitHub. Moreover, the FAR indicates that the specs need to be refined further, but there appears to be no active development on GitHub regarding this issue. Finally, JavaMOP supports Java 8 and JUnit 4, posing a significant hindrance to conducting such studies and adopting RV in a developer's day-to-day workflow.

To improve RV, it is imperative to actively maintain the responsible software (for instance, JavaMOP). Adding mined specs (after approval from the authors) regularly to such software will ensure that the software is up-to-date and accurate so that developers can access the most reliable version. Additionally, it will also potentially reduce the risk of developers relying on outdated or buggy specs. Finally, RV software should have CI/CD services support, making them much more accessible and feasible to developers, accelerating RV's wide-scale adoption.
